<!DOCTYPE HTML>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8">
  
  <title>Paul.ll</title>
  <meta name="author" content="Paul.ll">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Paul.ll"/>

  
    <meta property="og:image" content=""/>
  

  <link rel="shortcut icon" href="/github.io/favicon.png">
  
  
<link rel="stylesheet" href="/github.io/css/style.css">

  <!--[if lt IE 9]><script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script><![endif]-->
  

<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/github.io/">Paul.ll</a></h1>
  <h2><a href="/github.io/">坚持是一种态度！</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/github.io/null">Home</a></li>
    
      <li><a href="/github.io/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article id="post-00DateManage(时间管理)/OKR工作法" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2018-07-13T10:36:02.000Z"><a href="/github.io/2018/07/13/00DateManage(%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86)/OKR%E5%B7%A5%E4%BD%9C%E6%B3%95/">2018-07-13</a></time>
      
      
  
    <h1 class="title"><a href="/github.io/2018/07/13/00DateManage(%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86)/OKR%E5%B7%A5%E4%BD%9C%E6%B3%95/">OKR工作法</a></h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <!--TOC-->
<ul>
<li><a href="##1-OKR%E6%98%AF%E4%BB%80%E4%B9%88">1. OKR是什么</a></li>
<li><a href="##2-OKR%E6%89%A7%E8%A1%8C%E6%AD%A5%E9%AA%A4">2. OKR执行步骤</a></li>
<li><a href="##3-OKR%E5%B7%A5%E4%BD%9C%E6%B3%95%E7%9A%84%E7%90%86%E8%A7%A3">3. OKR工作法的理解</a></li>
<li><a href="##4-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8OKR%E5%B7%A5%E4%BD%9C%E6%B3%95">4. 如何使用OKR工作法</a></li>
</ul>
<!--/TOC-->

<p>【关键字】目标-关键结果、承担职责-庆祝成果</p>
<p><a id="markdown-1-OKR是什么" name="1-OKR是什么"></a></p>
<h2 id="1-OKR是什么"><a href="#1-OKR是什么" class="headerlink" title="1. OKR是什么"></a>1. OKR是什么</h2><p>OKR - Object-Key Result （目标-关键结果）</p>
<p>保护一个创意并不重要，重要的是保护把创意变为现实的过程。</p>
<p><a id="markdown-2-OKR执行步骤" name="2-OKR执行步骤"></a></p>
<h2 id="2-OKR执行步骤"><a href="#2-OKR执行步骤" class="headerlink" title="2. OKR执行步骤"></a>2. OKR执行步骤</h2><p>*** OKR执行步骤 ***<br>首先，设置有挑战、可衡量的阶段性目标。<br>其次，确保你和你的团队一直朝着这个目标前进，不要被其他事情干扰。<br>最后，把握节奏，所有成员一直明确需要努力达成的目标，并相互支持、相互鼓励。</p>
<p><a id="markdown-3-OKR工作法的理解" name="3-OKR工作法的理解"></a></p>
<h2 id="3-OKR工作法的理解"><a href="#3-OKR工作法的理解" class="headerlink" title="3. OKR工作法的理解"></a>3. OKR工作法的理解</h2><p>在《OKR工作法》通过一个创业团队的故事，讲述了OKR实施过程。通过该书可了解到：<br>a、OKR工作法是一个“目标管理”框架。<br>b、OKR推行过程中</p>
<p>1、设定目标（愿景） - 执行 - 检查 - 总结<br>2、目标执行周期：每周、每月、每季度、每年<br>3、任务分级别：必须完成、关注<br>4、设定目标原则：1）基础+难度 2）对“愿景”有推动作用 3）量化目标<br>5、如何推动目标：定期总结、回顾<br>6、对不同岗位对OKR的使用。“工作内容不一样，”关键结果“相同</p>
<p><a id="markdown-4-如何使用OKR工作法" name="4-如何使用OKR工作法"></a></p>
<h2 id="4-如何使用OKR工作法"><a href="#4-如何使用OKR工作法" class="headerlink" title="4. 如何使用OKR工作法"></a>4. 如何使用OKR工作法</h2>
      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article id="post-02MachineLearning(机器学习)/【机器学习实战】-01KNN近邻算法" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2018-07-13T10:36:02.000Z"><a href="/github.io/2018/07/13/02MachineLearning(%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%91-01KNN%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/">2018-07-13</a></time>
      
      
  
    <h1 class="title"><a href="/github.io/2018/07/13/02MachineLearning(%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%91-01KNN%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/">KNN近邻算法</a></h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <p>【博客的主要内容主要是自己的学习笔记，并结合个人的理解，供各位在学习过程中参考，若有疑问，欢迎提出；若有侵权，请告知博主删除，原创文章转载还请注明出处。】</p>
<p>#1.什么是kNN#<br>k-近邻算法（kNN k-NearestNeighbor）采用测量不同特征值之间的距离方法进行分类。在确定分类决策上仅依据最邻近的一个或多个样本的类别来决定待分样本所属类别。</p>
<p>#2.用途#<br>kNN算法主要被用于文本分类、相似推荐.</p>
<p>#3. kNN工作原理#<br>存在一个样本数据集合（即训练样本集），并且样本集中每个数据都存在标签（即每个数据与所属分类的对应关系）。输入没有标签的新数据之后，将新数据的每个特征与样本集中数据对应的特征进行比较，算法将提取出样本集中特征最相似数据（最近邻）的分类标签。</p>
<p>一般选择样本数据集中前K个最相似的数据，k一般不大于20的整数。</p>
<p><img src="http://i.imgur.com/mQ5PbNb.png"></p>
<p>说明：图中绿色圆点表示新数据，蓝色正方形和红色三角形表示已分类的样本集。<br>如果K=3，由于红色三角形所占比例为2/3，绿色圆点则属于“红色三角形”分类；<br>如果K=5，由于蓝色正方形所占比例为3/5，绿色圆点则属于“蓝色正方形”分类。</p>
<p>#4. kNN算法流程#<br>使用K-近邻算法将每组数据划分到某个类中：<br>     1）计算已知类别数据集中的点与当前点之间的距离；<br>            <img src="http://i.imgur.com/Fy5x30m.png"><br>     2）按照距离递增次序排序；<br>     3）选取与当前点距离最小的K个点；<br>     4）确定前K个点所在类别的出现频率；<br>     5）返回前K个点出现频率最高的类别作为当前点的预测分类。</p>
<p>#5. kNN算法Code#<br><strong>1.Python算法实现</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from numpy import *</span><br><span class="line">import  operator</span><br><span class="line">from os import listdir</span><br><span class="line"></span><br><span class="line">def classify0(inX, dataSet, labels,k):        </span><br><span class="line">    #0、获取数据Count</span><br><span class="line">    dataSetSize = dataSet.shape[0]</span><br><span class="line">    </span><br><span class="line">    #1、计算距离</span><br><span class="line">    diffMat = tile(inX,(dataSetSize,1)) - dataSet</span><br><span class="line">    sqDiffMat = diffMat ** 2</span><br><span class="line">    sqDistances = sqDiffMat.sum(axis=1)</span><br><span class="line">    distances = sqDistances ** 0.5</span><br><span class="line">    </span><br><span class="line">    #2、选择最小距离的k个点</span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    </span><br><span class="line">    #3、确定前K个点所在类别的出现频率</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    for i in range(k):</span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1</span><br><span class="line">    </span><br><span class="line">    #4、返回前K个点出现频率最高的类别作为当前点的预测分类</span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)</span><br><span class="line">    </span><br><span class="line">    return sortedClassCount[0][0]  </span><br></pre></td></tr></table></figure>

<p><strong>2、kNN算法测试</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def createDataSet():</span><br><span class="line">	group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])</span><br><span class="line">	labels = [&#x27;A&#x27;,&#x27;A&#x27;,&#x27;B&#x27;,&#x27;B&#x27;]</span><br><span class="line">	return group,labels</span><br></pre></td></tr></table></figure>

<p>#6. kNN算法特点#<br><strong>优点</strong><br>    1、简单，易于理解，易于实现，无需估计参数，无需训练；<br>    2、适合对稀有事件进行分类；<br>    3、适用于多分类问题。<br><strong>缺点</strong><br>    1、计算复杂度高、空间复杂度高；<br>     2、可理解性差，无法给出像决策树类的规则。</p>
<p>#7. kNN示例分析#<br>##7.1 示例1：使用k-近邻算法改进约会网站的配对效果##<br><strong>1.背景</strong><br>海伦通过在线约会网寻找自己合适的约会对象。根据自己一番总结将约会对象分为三类：<br>    1、不喜欢的人；<br>    2、魅力一般的人；<br>    3、极具魅力的人.</p>
<p>海伦收集网站中每位约会对象的以下数据来划分其约会类型：<br>    1、每年获得的飞行公里数；<br>    2、玩视频游戏所耗时间百分比；<br>    3、每周消费的冰淇淋公升数.</p>
<p>编写个小程序，海伦输入会员特征值程序显示该会员所属分类。</p>
<p><strong>2. 分析</strong><br>        1）收集数据：提供文本文件<br>         2）准备数据：使用Python解析文本文件<br>     3）分析数据：使用Matplotlib画二维扩散图<br>         4）训练数据：不适合kNN<br>         5）测试数据：使用海伦提供的部分数据作为测试样本<br>         6）使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断是否为自己喜欢的类型。</p>
<p><strong>3. Code</strong><br>    使用Python解析文本文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def file2matrix(filename):</span><br><span class="line">     fr = open(filename)</span><br><span class="line">     numberOfLines = len(fr.readlines())</span><br><span class="line">	     </span><br><span class="line">     #初始化矩阵</span><br><span class="line">     returnMat = zeros((numberOfLines,3))</span><br><span class="line">     </span><br><span class="line">     #初始化标签矩阵</span><br><span class="line">     classLabelVector = []</span><br><span class="line">     </span><br><span class="line">     fr = open(filename)</span><br><span class="line">     index = 0</span><br><span class="line">     </span><br><span class="line">     for line in fr.readlines():</span><br><span class="line">         line = line.strip()</span><br><span class="line">         listFromLine = line.split(&#x27;\t&#x27;)</span><br><span class="line">         returnMat[index,:] = listFromLine[0:3]</span><br><span class="line">         classLabelVector.append(int(listFromLine[-1]))</span><br><span class="line">         index += 1</span><br><span class="line">    return returnMat,classLabelVector</span><br></pre></td></tr></table></figure>
<p><strong>4. 使用Matplotlib画二维扩散图</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#读取样本数据   </span><br><span class="line">datingDataMat,datingLabels = file2matrix(&#x27;H:\workspacePy\machinelearninginaction\Ch02\datingTestSet2.txt&#x27;)</span><br><span class="line">	 		</span><br><span class="line">#散点图显示样本数据</span><br><span class="line">import matplotlib</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.font_manager </span><br><span class="line">	 		</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(111)</span><br><span class="line">	 		</span><br><span class="line">##1、散点图显示“玩视频游戏所耗时间百分比”、“每周消费的冰淇淋公升数”</span><br><span class="line">ax.scatter(datingDataMat[:,1], datingDataMat[:,2], 15.0*array(datingLabels),15.0*array(datingLabels))</span><br><span class="line">plt.show()</span><br><span class="line">	 		</span><br><span class="line">##2、散点图显示“每年获得的飞行常客里程数”、“玩视频游戏所耗时间百分比”</span><br><span class="line">ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*array(datingLabels),15.0*array(datingLabels))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://i.imgur.com/bJHYngU.png"></p>
<p><strong>5.归一化特征值</strong></p>
<p>特征值数值间相差较大，在计算距离时易产生较大偏差。例如：<br>        <img src="http://i.imgur.com/dljLODL.png"><br>”每年获得飞行常客里程数”远大于“玩视频游戏所耗时间百分比”和“每周消费冰淇淋公升数”，仅仅因为“每年获得飞行常客里程数”远大于其它2个特征值，计算距离结果将放大。我们将“每年获得飞行常客里程数”特征换算为“0到1”或“-1到1”之间，称为<strong>“归一化特征值”</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def autoNorm(dataSet):</span><br><span class="line">    minVals = dataSet.min(0)</span><br><span class="line">    maxVals = dataSet.max(0)</span><br><span class="line">    ranges = maxVals - minVals</span><br><span class="line">    normDataSet = zeros(shape(dataSet))</span><br><span class="line">    m = dataSet.shape[0]</span><br><span class="line">    normDataSet = dataSet - tile(minVals,(m,1))</span><br><span class="line">    normDataSet = normDataSet/tile(ranges,(m,1))</span><br><span class="line">    return normDataSet,ranges,minVals</span><br></pre></td></tr></table></figure>

<p> <strong>6.测试算法</strong></p>
<p>机器学习算法很大重要工作就是评估算法的正确率，通常将提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检查分类器的正确率。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def datingClassTest():</span><br><span class="line"> 	hoRatio = 0.10</span><br><span class="line">	datingDataMat,datingLabels = file2matrix(&#x27;datingTestSet.txt&#x27;)</span><br><span class="line"> 	normDataSet,ranges,minVals = autoNorm(datingDataMat)</span><br><span class="line">    m = normDataSet.shape[0]</span><br><span class="line">    numTestVecs = int(m * hoRatio)</span><br><span class="line">    errorCount = 0.0    </span><br><span class="line">    for i in range(numTestVecs):</span><br><span class="line">        classifierResult = classify0(normDataSet[i,:],normDataSet[numTestVecs:m,:],datingLabels[numTestVecs:m],3)</span><br><span class="line">        print &quot;the classifier came back with:%d, the real answer is: %d&quot; % (classifierResult,datingLabels[i])</span><br><span class="line">        if(classifierResult != datingLabels[i]):</span><br><span class="line">            errorCount += 1.0</span><br><span class="line"> 	    print &quot;the total error rate is : %f&quot; % (errorCount/float(numTestVecs))</span><br></pre></td></tr></table></figure>
<p>##7.2 示例2：手写识别系统##<br><strong>1. 背景</strong><br>        数字0<del>9，通过32*32位图来展现。现有1900个位图数据，分别表示了0</del>9数字内容。需通过kNN方法能快速分辨出位图的数字。<br><strong>2.分析</strong><br>        1）收集数据：提供文本文件；<br>        2）准备数据：将图像格式转换为分类器使用的list格式；<br>        3）分析数据：在Python命令提示符中检查数据，确保它符合要求；<br>        4）训练算法：不适合kNN算法；<br>        5）测试数据：编写函数使用提供的部分数据集作为测试样本，测试样本与非测试样本的区别在于测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。<br>        6）使用算法：省略</p>
<p><strong>3.Code</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#将32**32位图片数据转换为1*1024的list</span><br><span class="line">def img2vector(filename):</span><br><span class="line">    returnVect = zeros((1,1024))</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    for i in range(32):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        for j in range(32):</span><br><span class="line">            returnVect[0,32*i+j] = int(lineStr[j])</span><br><span class="line">    return returnVect</span><br></pre></td></tr></table></figure>

<p><strong>4.测试算法</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#测试算法</span><br><span class="line">def handwritingClassTest():</span><br><span class="line">    #1、获取样本数据集合，</span><br><span class="line">    hwLabels = []</span><br><span class="line">    path = &#x27;H:\\workspacePy\\machinelearninginaction\\Ch02\\digits\\trainingDigits&#x27;</span><br><span class="line">    trainingFileList = listdir(path)</span><br><span class="line">    m = len(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m,1024))</span><br><span class="line">    </span><br><span class="line">    #2、将样本数据并转换为list</span><br><span class="line">    for i in range(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(&#x27;.&#x27;)[0]</span><br><span class="line">        classNumStr = int(fileStr.split(&#x27;_&#x27;)[0])</span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        </span><br><span class="line">        trainingMat[i,:] = img2vector(path + &#x27;/%s&#x27; % fileNameStr)</span><br><span class="line">                </span><br><span class="line">    #3、使用测试样本进行测试对比。</span><br><span class="line">    path1 = &#x27;H:\\workspacePy\\machinelearninginaction\\Ch02\\digits\\testDigits&#x27;</span><br><span class="line">    testFileList = listdir(path1)</span><br><span class="line">    errorCount = 0.0</span><br><span class="line">    mTest = len(testFileList)</span><br><span class="line">    for i in range(mTest):</span><br><span class="line">        fileNameStr = testFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(&#x27;.&#x27;)[0]</span><br><span class="line">        classNumStr = int(fileStr.split(&#x27;_&#x27;)[0])</span><br><span class="line">        </span><br><span class="line">        vectorUnderTest = img2vector(path1 + &#x27;/%s&#x27; % fileNameStr)</span><br><span class="line">        </span><br><span class="line">        ##knn</span><br><span class="line">        classifierResult = classify0(vectorUnderTest,trainingMat,hwLabels,3)</span><br><span class="line">        print &quot;the classifier came back with:%d,the real answer is :%d&quot; %(classifierResult,classNumStr)</span><br><span class="line">        if(classifierResult != classNumStr): errorCount +=1.0</span><br><span class="line">    print &quot;\nthe total number of errors is :%d&quot; % errorCount</span><br><span class="line">    print &quot;\nthe total error rate is:%f&quot; % (errorCount/float(mTest))</span><br></pre></td></tr></table></figure>



      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article id="post-02MachineLearning(机器学习)/【机器学习实战】-02决策树ID3" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2018-07-13T10:36:02.000Z"><a href="/github.io/2018/07/13/02MachineLearning(%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%91-02%E5%86%B3%E7%AD%96%E6%A0%91ID3/">2018-07-13</a></time>
      
      
  
    <h1 class="title"><a href="/github.io/2018/07/13/02MachineLearning(%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%91-02%E5%86%B3%E7%AD%96%E6%A0%91ID3/">决策树ID3</a></h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <p>【博客的主要内容主要是自己的学习笔记，并结合个人的理解，供各位在学习过程中参考，若有疑问，欢迎提出；若有侵权，请告知博主删除，原创文章转载还请注明出处。】</p>
<h1 id="1-什么是决策树"><a href="#1-什么是决策树" class="headerlink" title="1.什么是决策树"></a>1.什么是决策树</h1><p><strong>决策树</strong>是一种决策支持工具，通过使用树型图方式（可以是二叉树或非二叉树）展示尽可能的结果。</p>
<p>##1.1 决策规则<br>决策树是线性决策，其每个非叶节点表示一个特征属性上的判断，叶节点表示出现的结果。</p>
<p>决策树的决策过程（即每个分支）是从决策树的根节点开始，待测数据与决策树中的特征节点进行比较，并按照比较结果选择下一个比较节点，直到叶节点作为最终的决策结果。可以表示为：<br><strong>If 条件1 and 条件2 and 条件3 then 结果</strong></p>
<p>##1.2决策树绘制</p>
<ul>
<li>决策节点：使用矩形框表示；     </li>
<li>机会节点：使用圆圈表示；  </li>
<li>终结点：使用三角形表示。</li>
</ul>
<p>##1.3决策树的经典算法<br>    ID3、C4.5、CART</p>
<h1 id="2-决策树的建立"><a href="#2-决策树的建立" class="headerlink" title="2. 决策树的建立"></a>2. 决策树的建立</h1><ul>
<li>特征选择：从训练数据的特征中选择一个特征作为当前节点的分裂标准</li>
<li>决策树生成：依据选择特征标准，从上至下递归生成子节点，直到数据集不可再分则停止。</li>
<li>剪枝：决策树易过拟合，需要通过剪枝来缩小树的结构和规模。</li>
</ul>
<h1 id="3-决策树-ID3算法"><a href="#3-决策树-ID3算法" class="headerlink" title="3. 决策树-ID3算法"></a>3. 决策树-ID3算法</h1><h2 id="3-1-特征选择"><a href="#3-1-特征选择" class="headerlink" title="3.1 特征选择"></a>3.1 特征选择</h2><p><strong>决策树的目标</strong>是将数据集按对应类标签进行分类。通过特征的选择将不同类别的数据集“赋予”对应的标签。特征选择的目标使得分类后的数据更佳“有序”。如何衡量一个数据集“有序”程度（或称纯度），通过数据纯度函数来判断，这里介绍两种纯度函数：</p>
<dl><dt>1.0 信息增益（information gain)</dt><dd><strong>信息熵(entropy)</strong> 表示信息的期望（即不确定程度）。当选择某个特征对数据集进行分类，划分前后数据集信息熵的差值表示为<strong>信息增益</strong>。信息增益衡量某个特征对分类结果的影响大小。<br>假设：训练数据集为D，数据类别数为c。在构建决策树时，根据某个特征对数据集进行分类。在此，计算出该数据中的信息熵：</dd></dl><p>:    <strong>A.分类前的信息熵计算公式</strong><br>$$Infos(D) = -\sum_{i=1}^n p(i) log_2p(i)$$ Pi表示类别i样本数量占所有样本的比例。</p>
<p>:    <strong>B.分类后的信息熵计算公式</strong><br>对应数据集D，选择特征A作为决策树判断节点时，在特征A作用后信息熵为Infosa(D)，其计算公式如下：<br>$$InfosA(D) = -\sum_{j=1}^k \frac {|Dj|} {|D|} * Infos(Dj) $$K表示顺联数据集D被划分为k个部分。</p>
<p>:     <strong>C.数据集的信息增益</strong><br>信息增益表示数据集D在特征A的作用后，其信息熵减少的值，计算公式如下：<br>$$Gain(A)=Infos(D)−Infosa(D)$$</p>
<dl><dt>2.0 基尼不纯度（Gini impurity)</dt><dd>从一个数据集中随机选取子项，度量其被错误分类到其它分组里的概率。</dd></dl><p>:     <strong>A.分类前信息熵计算公式</strong><br>$$Gini(D) = 1 - \sum_{i=1} ^ c  \left( pi \right)^2 $$c表示数据集中类别的数量；pi表示类别i样本数量所占样本比例。</p>
<p>:     <strong>B.分类后信息熵计算公式</strong><br>$$GiniA(D) = \sum_{j=1} ^ k  \frac{|Dj|}{ |D| } Gini(Dj) $$</p>
<p>:     <strong>C.数据集的信息增益</strong><br>$$∆Gini(A)=Ginia(D)−Gini(D)$$         </p>
<p><strong>选择增益最大的特征作为该节点的分裂条件。</strong></p>
<h2 id="3-2-剪枝"><a href="#3-2-剪枝" class="headerlink" title="3.2 剪枝"></a>3.2 剪枝</h2><p>在分类模型建立过程中，容易出现过拟合情况。</p>
<p><strong>过拟合</strong>是指在模型学习训练中，训练样本精确度非常高，导致非训练样本的精确度的误差随着训练次数先下降后上升的现象。</p>
<p>标准定义：给定一个假设空间H，一个假设h属于H，如果存在其他的假设h’属于H,使得在训练样例上h的错误率比h’小，但在整个实例分布上h’比h的错误率小，那么就说假设h过度拟合训练数据。 —-《Machine Learning》Tom M.Mitchell</p>
<dl><dt>决策树的拟合现象通过“剪枝”技术做一定的修复。<strong>“剪枝”分为“预剪枝”和“后剪枝”</strong>。</dt><dd><strong>预剪枝</strong>：在决策树创建过程中，算法中加入一定的限制条件来终止树的生长，以避免过拟合度。通常的方法，信息增益小于一定阀值的请示后通过剪枝使决策树停止生长。阀值设置不当导致模型拟合不足和过拟合情况。</dd></dl><p>:     <strong>后剪枝</strong>：在决策树生长之后，按照自下而上的方式进行修剪决策树。通常两种方式：一种用新的叶节点来替换分支；另一种用最常使用分支来替换。</p>
<h2 id="3-3-优点和缺点"><a href="#3-3-优点和缺点" class="headerlink" title="3.3 优点和缺点"></a>3.3 优点和缺点</h2><p><strong>优点</strong>：计算复杂度不高，输出结构易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。<br><strong>缺点</strong>：可能会产生过度匹配问题。</p>
<h1 id="4-决策树-ID3算法实现：基于python-和-numpy"><a href="#4-决策树-ID3算法实现：基于python-和-numpy" class="headerlink" title="4. 决策树-ID3算法实现：基于python 和 numpy"></a>4. 决策树-ID3算法实现：基于python 和 numpy</h1><p>示例：<br><img src="http://img.blog.csdn.net/20160621130335078" alt="这里写图片描述"></p>
<h2 id="4-1-算法伪代码"><a href="#4-1-算法伪代码" class="headerlink" title="4.1 算法伪代码"></a>4.1 算法伪代码</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CreateBranch()</span><br><span class="line">	检测数据集中的每个子项是否属于同一分类：</span><br><span class="line">	If so return 类标签</span><br><span class="line">	Else</span><br><span class="line">		寻找划分数据集的最好特征</span><br><span class="line">		划分数据集</span><br><span class="line">		创建分支节点</span><br><span class="line">		For 每个划分的子集</span><br><span class="line">			调用函数createBranch并增加返回结果到分支节点中</span><br><span class="line">	Return 分支节点</span><br></pre></td></tr></table></figure>
<h2 id="4-2-算法实现"><a href="#4-2-算法实现" class="headerlink" title="4.2 算法实现"></a>4.2 算法实现</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created on Sat Jun 18 16:50:40 2016</span><br><span class="line">@author: Paul.lu</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">from math import log</span><br><span class="line">import operator</span><br><span class="line"></span><br><span class="line">#创建样本数据集和标签</span><br><span class="line">def createDataSet():</span><br><span class="line">    dataSet = [[1,1,&#x27;yes&#x27;],</span><br><span class="line">        [1,1,&#x27;yes&#x27;],</span><br><span class="line">        [1,0,&#x27;no&#x27;],</span><br><span class="line">        [0,1,&#x27;no&#x27;],</span><br><span class="line">        [0,1,&#x27;no&#x27;]]</span><br><span class="line">    labels = [&#x27;no surfacing&#x27;,&#x27;flippers&#x27;]</span><br><span class="line">    return dataSet,labels</span><br><span class="line">    </span><br><span class="line">dataSet,labels = createDataSet()</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">#说明：计算特征值的熵</span><br><span class="line">#返回：熵值</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">def calcShannonEnt(dataSet):</span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    for featVec in dataSet:</span><br><span class="line">        currentLabel = featVec[-1]</span><br><span class="line">        if currentLabel not in labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = 0</span><br><span class="line">        labelCounts[currentLabel] += 1</span><br><span class="line">        </span><br><span class="line">    #计算“熵”</span><br><span class="line">    shannonEnt = 0.0</span><br><span class="line">    for key in labelCounts:</span><br><span class="line">        prob = float(labelCounts[key]) / numEntries</span><br><span class="line">        shannonEnt -= prob * log(prob,2)</span><br><span class="line">    return shannonEnt</span><br><span class="line"></span><br><span class="line">#shannonEnt = calcShannonEnt(dataSet)</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">#说明：划分数据集</span><br><span class="line">#splitDataSet(待划分的数据集，划分数据集的特征，特征的返回值)</span><br><span class="line">#返回：划分数据集</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">def splitDataSet(dataSet,axis,value):</span><br><span class="line">    retDataSet = []</span><br><span class="line">    for featVec in dataSet:</span><br><span class="line">        if featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]</span><br><span class="line">            reducedFeatVec.extend(featVec[axis+1:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    return retDataSet</span><br><span class="line">    </span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">说明：选择最好的数据划分方式</span><br><span class="line">1、计算信息增益：Gain（A） = Info(D) - InfoA(D)</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;       </span><br><span class="line">def chooseBestFeatureToSplit(dataSet):</span><br><span class="line">    #获得标签个数</span><br><span class="line">    numFeatures = len(dataSet[0]) - 1</span><br><span class="line">    </span><br><span class="line">    #计算“数据集拆分前信息熵”</span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    </span><br><span class="line">    #计算“数据集拆分后信息熵”</span><br><span class="line">    bestInfoGain = 0.0; bestFeature = -1</span><br><span class="line">    for i in range(numFeatures):</span><br><span class="line">        featList = [example[i] for example in dataSet]</span><br><span class="line">        uniqueVals = set(featList)</span><br><span class="line">        newEntropy = 0.0</span><br><span class="line">    </span><br><span class="line">        for value in uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet,i,value)</span><br><span class="line">            prob = len(subDataSet) / float(len(dataSet))</span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">            </span><br><span class="line">    #计算“信息增益”</span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        </span><br><span class="line">    #获得“信息增益”最大值</span><br><span class="line">        if(infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">            </span><br><span class="line">    return bestFeature</span><br><span class="line"></span><br><span class="line">#chooseBestFeatureToSplit(dataSet)</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">#说明：如果数据集已经处理了所有属性，但是类标签依然不是唯一的，对该类叶子节点，通过多数表决来确定该叶子节点的分类。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">def majorityCnt(classList):</span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    for vote in classList:</span><br><span class="line">        if vote not in classCount.keys(): classCount[vote] = 0</span><br><span class="line">        classCount[vote] += 1</span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)</span><br><span class="line">    return sortedClassCount</span><br><span class="line">    </span><br><span class="line">&#x27;&#x27;&#x27;    </span><br><span class="line">#创建决策树</span><br><span class="line">#输入：样本数据集、标签列表</span><br><span class="line">#返回：决策树</span><br><span class="line">#说明：</span><br><span class="line">1、递归终止条件：</span><br><span class="line">    程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。</span><br><span class="line">    </span><br><span class="line">2、选择最好的数据集划分；</span><br><span class="line">    2.1 计算数据集划分前“信息熵”</span><br><span class="line">    2.2 计算数据集划分后“信息熵”</span><br><span class="line">    2.3 计算“信息增益” = Info(D) - Infoa(D)</span><br><span class="line">    2.4 获得最大的“信息增益”，以获得最好的数据划分。</span><br><span class="line">    </span><br><span class="line">3、遍历特征值，创建分支。</span><br><span class="line"></span><br><span class="line">返回数据集示例：</span><br><span class="line">最终返回数据集dict形式的数据：</span><br><span class="line">    &#123;&#x27;no surfacing&#x27;: &#123;0: &#x27;no&#x27;, 1: &#123;&#x27;flippers&#x27;: &#123;0: &#x27;no&#x27;, 1: &#x27;yes&#x27;&#125;&#125;&#125;&#125;</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">def createTree(dataSet,labels):</span><br><span class="line">    classList = [example[-1] for example in dataSet]</span><br><span class="line">    &#x27;&#x27;&#x27;递归终止的条件：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。&#x27;&#x27;&#x27;</span><br><span class="line">    if classList.count(classList[0]) == len(classList):</span><br><span class="line">        return classList[0]</span><br><span class="line"></span><br><span class="line">    if len(dataSet[0]) == 1:</span><br><span class="line">        return majorityCnt(classList)</span><br><span class="line">    </span><br><span class="line">    &#x27;&#x27;&#x27;选择最好的数据集划分&#x27;&#x27;&#x27;</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    </span><br><span class="line">    &#x27;&#x27;&#x27;对特征值进行遍历，创建分支&#x27;&#x27;&#x27;</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    del(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] for example in dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    </span><br><span class="line">    for value in uniqueVals:</span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels)</span><br><span class="line">        </span><br><span class="line">    return myTree</span><br><span class="line">    </span><br><span class="line">ct = createTree(dataSet,labels)</span><br><span class="line">print ct</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>【参考】</strong></p>
<ul>
<li>什么是决策树 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Decision_tree">https://en.wikipedia.org/wiki/Decision_tree</a></li>
<li>决策树学习  <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></li>
<li>决策树算法介绍及应用 <a target="_blank" rel="noopener" href="http://www.ibm.com/developerworks/cn/analytics/library/ba-1507-decisiontree-algorithm/index.html">http://www.ibm.com/developerworks/cn/analytics/library/ba-1507-decisiontree-algorithm/index.html</a></li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article id="post-06ReadWrite(读书写作)/20180110Markdown介绍及入门" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2018-01-10T10:36:02.000Z"><a href="/github.io/2018/01/10/06ReadWrite(%E8%AF%BB%E4%B9%A6%E5%86%99%E4%BD%9C)/20180110Markdown%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%85%A5%E9%97%A8/">2018-01-10</a></time>
      
      
  
    <h1 class="title"><a href="/github.io/2018/01/10/06ReadWrite(%E8%AF%BB%E4%B9%A6%E5%86%99%E4%BD%9C)/20180110Markdown%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%85%A5%E9%97%A8/">Markdown介绍及入门</a></h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <p>&lt;–TOC–&gt;</p>
<ul>
<li>[1. Markdown 是什么](#1-Markdown 是什么)</li>
<li>[2. Markdown 的特点](#2-Markdown 的特点)</li>
<li><a href="#3-MarkDown%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95">3. MarkDown基本语法</a><!--TOC--></li>
</ul>
<p><img src="./Markdown%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95pic/001.png" alt="Markdown">{:height=”80%” width=”80%” align=”center”}</p>
<p><a id="markdown-1-Markdown 是什么" name="1-Markdown 是什么"></a></p>
<h1 id="一、Markdown-是什么"><a href="#一、Markdown-是什么" class="headerlink" title="一、Markdown 是什么"></a>一、Markdown 是什么</h1><p>Markdown 的目标是实现「易读易写」</p>
<p>可读性，无论如何，都是最重要的。</p>
<p>一份使用 Markdown 格式撰写的文件应该可以直接以纯文本发布，并且看起来不会像是由许多标签或是格式指令所构成。Markdown 语法受到一些既有 text-to-HTML 格式的影响，包括 Setext、atx、Textile、reStructuredText、Grutatext和EtText，而最大灵感来源其实是纯文本电子邮件的格式。</p>
<p>总之， Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然。比如：在文字两旁加上星号，看起来就像<em>强调</em>。Markdown 的列表看起来，嗯，就是列表。Markdown 的区块引用看起来就真的像是引用一段文字，就像你曾在电子邮件中见过的那样。</p>
<p><a id="markdown-2-Markdown 的特点" name="2-Markdown 的特点"></a></p>
<h1 id="二、Markdown-的特点"><a href="#二、Markdown-的特点" class="headerlink" title="二、Markdown 的特点"></a>二、Markdown 的特点</h1><h2 id="1-兼容HTML"><a href="#1-兼容HTML" class="headerlink" title="1. 兼容HTML"></a>1. 兼容HTML</h2><p>Markdown语法的目标是：成为一种适用于网络的书写语言。</p>
<p>Markdown的理念是，能让文档更容易读、写和随意改，Markdown是一种书写的格式。</p>
<p>Markdown的格式语法只涵盖纯文本可以涵盖的范围，不在范围内的标签可以直接在文档中通过HTML编写。而不需要而外标注是“HTML”或“Markdown”语法标识。</p>
<p>在HTML区块标签间的Markdown格式语法将不会被处理。HTML 的区段（行内）标签如&lt;span&gt;、&lt;cite&gt;、&lt;del&gt; 可以在 Markdown 的段落、列表或是标题里随意使用。</p>
<p>要制约的只有一些 HTML 区块元素比如&lt;div&gt;、&lt;table&gt;、&lt;pre&gt;、&lt;p&gt;等标签，必须在前后加上空行与其它内容区隔开，还要求它们的开始标签与结尾标签不能用制表符或空格来缩进</p>
<h2 id="2-特殊字符自动转换"><a href="#2-特殊字符自动转换" class="headerlink" title="2. 特殊字符自动转换"></a>2. 特殊字符自动转换</h2><p>在 HTML 文件中，有两个字符需要特殊处理： &lt; 和 &amp; 。 &lt; 符号用于起始标签，&amp; 符号则用于标记 HTML 实体。</p>
<p>不过需要注意的是，code 范围内，不论是行内还是区块， &lt; 和 &amp; 两个符号都一定会被转换成 HTML 实体。</p>
<p><a id="markdown-3-MarkDown基本语法" name="3-MarkDown基本语法"></a></p>
<h1 id="三、MarkDown基本语法"><a href="#三、MarkDown基本语法" class="headerlink" title="三、MarkDown基本语法"></a>三、MarkDown基本语法</h1><h2 id="1-段落、标题、区块代码"><a href="#1-段落、标题、区块代码" class="headerlink" title="1. 段落、标题、区块代码"></a>1. 段落、标题、区块代码</h2><ul>
<li><p>标题<br>Markdown支持两种标题的语法，<strong>Setext和atx</strong></p>
<blockquote>
<p><strong>Setext形式</strong> 是用底线的形式，利用“=”（最高阶标题）和“-”（第二阶标题）<br><strong>Atx形式</strong> 在行首插入1到6个#，对应到标题1到6阶</p>
</blockquote>
</li>
<li><p>区块引用</p>
<blockquote>
<p>使用email形式的**”&gt;”角括号**</p>
</blockquote>
</li>
</ul>
<h2 id="2-修辞和强调"><a href="#2-修辞和强调" class="headerlink" title="2. 修辞和强调"></a>2. 修辞和强调</h2><p>MarkDown使用 星号*和底线_ 来标记需要强调的区段。</p>
<h2 id="3-列表"><a href="#3-列表" class="headerlink" title="3. 列表"></a>3. 列表</h2><ul>
<li>无序列表使用星号*、加号+和减号-来做为列表的项目标记。</li>
<li>在项目之间插入空行，那项目的内容会用&lt;p&gt;包起来。</li>
<li>在项目之间放入多个段落，在段落前加4个空白或1个tab。</li>
</ul>
<h2 id="4-链接"><a href="#4-链接" class="headerlink" title="4. 链接"></a>4. 链接</h2><p>Markdown支持两种链接语法：行内和参考<br><strong>行内</strong></p>
<blockquote>
<p>[内容](引用链接地址 “标题”)</p>
</blockquote>
<p><strong>参考</strong></p>
<blockquote>
<p>[内容][数字]</p>
</blockquote>
<h2 id="5-图片"><a href="#5-图片" class="headerlink" title="5. 图片"></a>5. 图片</h2><p><strong>行内形式</strong></p>
<blockquote>
<p>![alt text](/path/to/img.jpg “title”)</p>
</blockquote>
<p><strong>参考形式</strong></p>
<blockquote>
<p>![alt text][id]<br>[id]:/path/to/img.jpg “title”</p>
</blockquote>
<h2 id="6-代码"><a href="#6-代码" class="headerlink" title="6. 代码"></a>6. 代码</h2><p>**反引号`**来标记代码区段<br>代码区段内的&amp;、&lt; 和 &gt; 都会被自动的转换成html实体</p>
<p>#四. 参考资料</p>
<p>[1]: <a target="_blank" rel="noopener" href="https://github.com/riku/Markdown-Syntax-CN/blob/master/syntax.md">Markdown 语法说明 (简体中文版)</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article id="post-05Version(版本管理)/GitABC" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2016-03-20T02:31:23.000Z"><a href="/github.io/2016/03/20/05Version(%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86)/GitABC/">2016-03-20</a></time>
      
      
  
    <h1 class="title"><a href="/github.io/2016/03/20/05Version(%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86)/GitABC/">Git学习笔记</a></h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <span id="more"></span>

<p><img src="/images/GitABC/dXQM4aU.png" alt="Git"></p>
<h2 id="专用名词"><a href="#专用名词" class="headerlink" title="专用名词"></a><strong>专用名词</strong></h2><pre><code>1.  workspace:工作区
2.  Index / Stage :暂存区
3.  Repository：仓库区（或本地仓库）
4.  Remote：远程仓库
</code></pre>
<h2 id="一、集中式和分布式"><a href="#一、集中式和分布式" class="headerlink" title="一、集中式和分布式"></a>一、集中式和分布式</h2><pre><code>集中式版本控制，即建立中央服务器，在中央服务器保管所有版本控制内容。若你需要对某个内容进行修改时，需从“中央服务器”先提取至本地，在完成修改后，重新提交至“中央服务器”。常见的有SVN，CVS，VSS。

分布式版本控制，即每个本地版本含有全套内容（每个本地版本即为中央服务器）。git属于分布式版本控制系统。
</code></pre>
<h2 id="二、安装GIT"><a href="#二、安装GIT" class="headerlink" title="二、安装GIT"></a>二、安装GIT</h2><pre><code>msysgit是Windows版的Git，从http://msysgit.github.io/下载，然后按默认选项安装即可.
</code></pre>
<h2 id="三、基本内容"><a href="#三、基本内容" class="headerlink" title="三、基本内容"></a>三、基本内容</h2><h3 id="3-1-创建版本库"><a href="#3-1-创建版本库" class="headerlink" title="3.1 创建版本库"></a>3.1 创建版本库</h3><p>1、初始化一个Git仓库，使用”git init”</p>
<p>2、添加文件至Git仓库：</p>
<pre><code>a、使用git add&lt;file&gt;,PS:可以反复多次使用，添加多个文件
b、使用git commit，完成提交。
    
</code></pre>
<h3 id="3-2-版本回退"><a href="#3-2-版本回退" class="headerlink" title="3.2 版本回退"></a>3.2 版本回退</h3><pre><code>git status 理解仓库当前状态
git diff 查看修改内容
-&gt;HEAD 指向的版本就是当前版本。
git log  查看提交历史
git log --pretty=oneline

git relog  查看命令历史
git reset --hard HEAD    回退版本
</code></pre>
<h3 id="3-3-提交修改"><a href="#3-3-提交修改" class="headerlink" title="3.3 提交修改"></a>3.3 提交修改</h3><pre><code>git add 将工作区第一次修改的内容放入暂存区，准备提交
git commit  只负责将暂存区内容提交至
</code></pre>
<h3 id="3-4-撤销修改"><a href="#3-4-撤销修改" class="headerlink" title="3.4 撤销修改"></a>3.4 撤销修改</h3><pre><code>git checkout -- &lt;file&gt; 将工作区的修改全部撤销
如果修改内容尚未放到“暂存区”，则回退到版本库版本；
如果修改内容之前提交到“暂存区”，又做了修改，则撤销修改将回到“暂存区”版本
    
</code></pre>
<h3 id="3-5-删除文件"><a href="#3-5-删除文件" class="headerlink" title="3.5 删除文件"></a>3.5 删除文件</h3><pre><code>git rm &lt;file&gt; 删除版本库文件
如果误删文件，可通过git checkout恢复之前版本。
</code></pre>
<h2 id="四、远程仓库管理"><a href="#四、远程仓库管理" class="headerlink" title="四、远程仓库管理"></a>四、远程仓库管理</h2><pre><code>Git是分布式版本控制系统，同一个Git仓库，可以分布到不同的机器上。
gitHub.com是一个免费git仓库托管服务网站。
</code></pre>
<h3 id="4-1-添加远程库"><a href="#4-1-添加远程库" class="headerlink" title="4.1 添加远程库"></a>4.1 添加远程库</h3><pre><code>a、关联远程库
    git remote add origin git@server-name:path/repo-name.git;
b、第一次推送master
    git push -u origin master
c、再次推送
    git push origin master
    git push [remote] --force #强行推送当前分支到远程仓库，即使有冲突
    git push [remote] --all#推送所有分支到远程仓库
    
</code></pre>
<h3 id="4-2-从远程库克隆"><a href="#4-2-从远程库克隆" class="headerlink" title="4.2 从远程库克隆"></a>4.2 从远程库克隆</h3><pre><code>git clone git@github.com:xxxxx/learnPython.git
</code></pre>
<h3 id="4-3-其它命令"><a href="#4-3-其它命令" class="headerlink" title="4.3 其它命令"></a>4.3 其它命令</h3><pre><code>git fetch [remote]  #下载远程仓库的所有变动
git remote -v 显示所有远程仓库
git remote show #显示某个远程仓库的信息
git pull [remote][branch] 取回远程仓库的变化，并与本地分支合并
</code></pre>
<h2 id="五、分支管理"><a href="#五、分支管理" class="headerlink" title="五、分支管理"></a>五、分支管理</h2><h3 id="5-1-创建与合并分支"><a href="#5-1-创建与合并分支" class="headerlink" title="5.1 创建与合并分支"></a>5.1 创建与合并分支</h3><ul>
<li><pre><code> 查看分支：git brach
</code></pre>
</li>
<li><pre><code> 创建分支：git brache &lt;name&gt;
</code></pre>
</li>
<li><pre><code> 切换分支：git checkout &lt;name&gt;
</code></pre>
</li>
<li><pre><code> 创建+切换分支： git checkout -b &lt;name&gt;
</code></pre>
</li>
<li><pre><code> 合并某分支到当前分支： git merge &lt;name&gt;
</code></pre>
</li>
<li><pre><code> 删除分支： git branch -d &lt;name&gt;
</code></pre>
</li>
</ul>
<h3 id="5-2-解决冲突"><a href="#5-2-解决冲突" class="headerlink" title="5.2 解决冲突"></a>5.2 解决冲突</h3><pre><code>各个版本及分支都有修改主要在master和支线间。当git无法自动合并分支时，先人工解决冲突，再提交，完成合并。

**git log --graph 可以看到分支合并图**
</code></pre>
<h3 id="5-3-分支管理策略"><a href="#5-3-分支管理策略" class="headerlink" title="5.3 分支管理策略"></a>5.3 分支管理策略</h3><pre><code>使用Fast forward模式进行合并分支，在这种模式下，删除分支后，将清除分支信息。
git merge --no-ff -m &quot;content&quot; &lt;branch&gt;
</code></pre>
<h3 id="5-4-Bug分支"><a href="#5-4-Bug分支" class="headerlink" title="5.4 Bug分支"></a>5.4 Bug分支</h3><pre><code>git stash 将当前工作现场“储藏”起来。
git stash list 查看封存的工作现场
恢复工作现场
    一种：git stash apply  #需要删除stash内容
        git stash drop
    二种：git stash pop
    
</code></pre>
<h3 id="5-5-Feature-分支"><a href="#5-5-Feature-分支" class="headerlink" title="5.5 Feature 分支"></a>5.5 Feature 分支</h3><pre><code>已经提交分支，进行强行删除：git branch -d &lt;branchName&gt;
</code></pre>
<h3 id="5-6-多人协作"><a href="#5-6-多人协作" class="headerlink" title="5.6 多人协作"></a>5.6 多人协作</h3><pre><code>1、查看远程库信息，git remove -v
2、从本地推送分支git push origin branch-name ,如果推送失败，则先用 git pull 抓取远程的新提交文件；
3、本地创建和远程分支：git checkout -b branch-name origin/branch-name
4、建立本地分支和远程分支的关联：git branch --set-upstream branch-name origin/branch-name;
</code></pre>
<h2 id="六、标签管理"><a href="#六、标签管理" class="headerlink" title="六、标签管理"></a>六、标签管理</h2><pre><code>1、查看所有标签：git tag
2、创建标签：git tag &lt;name&gt; &lt;commit id&gt;
3、查看标签信息：git show &lt;tagname&gt;
4、删除标签：git tag -d &lt;tag-name&gt;
5、推送标签：git push origin &lt;tag-name&gt;
6、推送全部未推送的本地标签：git push origin --tags
7、删除远程标签：git push origin :refs/tags/&lt;tagname&gt;
</code></pre>
<h2 id="七、忽略特殊文件"><a href="#七、忽略特殊文件" class="headerlink" title="七、忽略特殊文件"></a>七、忽略特殊文件</h2><pre><code>需要忽略某些文件是，编写.gitignore
.gitignore文件本身需要放在版本库中
</code></pre>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="as_sitesearch" value="qq17908/github.io">
  </form>
</div>


  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/github.io/categories/00DateManage-%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/">00DateManage(时间管理)</a><small>1</small></li>
  
    <li><a href="/github.io/categories/02MachineLearning-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">02MachineLearning(机器学习)</a><small>2</small></li>
  
    <li><a href="/github.io/categories/05Version-%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/">05Version(版本管理)</a><small>1</small></li>
  
    <li><a href="/github.io/categories/06ReadWrite-%E8%AF%BB%E4%B9%A6%E5%86%99%E4%BD%9C/">06ReadWrite(读书写作)</a><small>1</small></li>
  
  </ul>
</div>


  
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2021 Paul.ll
  
</div>
<div class="clearfix"></div></footer>
  
<script src="/github.io/js/jquery-3.4.1.min.js"></script>


<script src="/github.io/js/jquery.imagesloaded.min.js"></script>


<script src="/github.io/js/gallery.js"></script>






<link rel="stylesheet" href="/github.io/fancybox/jquery.fancybox.css">


<script src="/github.io/fancybox/jquery.fancybox.pack.js"></script>

<script>
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
